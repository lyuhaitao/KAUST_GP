# AUTOGENERATED! DO NOT EDIT! File to edit: ../V00/00_core.ipynb.

# %% auto 0
__all__ = ['grid_subsampling', 'Lht_VariationalStrategy', 'MaternFunction_old', 'MaternFunction', 'AFirstKernel_old',
           'AFirstKernel', 'Kernel_HalfNorm', 'Kernel_HalfNorm_old', 'StandardApproximateGP_mat', 'GP_AFirst',
           'grid_dataset', 'findNeighbors_old', 'findNeighbors']

# %% ../V00/00_core.ipynb 2
import numpy as np
import torch
import gpytorch
import torch.nn as nn
from gpytorch.models import ApproximateGP
from gpytorch.variational import CholeskyVariationalDistribution, NaturalVariationalDistribution, TrilNaturalVariationalDistribution
from gpytorch.variational import VariationalStrategy

from gpytorch.constraints import Positive
from gpytorch.settings import trace_mode, _linalg_dtype_cholesky
import scipy
from scipy.special import kv, kvp, digamma

from linear_operator.utils.cholesky import psd_safe_cholesky
from linear_operator.settings import cholesky_jitter
import linear_operator.settings as los
import linear_operator
from gpytorch.utils.memoize import cached
from linear_operator import to_dense
import tqdm

# %% ../V00/00_core.ipynb 3
def grid_subsampling(points, voxel_size):
    "Define a function that takes as input an array of points, and a voxel size expressed in meters"
    "points: it is from a LiDAR data; voxel_size: the length of edge"
    coords = points[:,0:2]
    nb_vox=np.ceil((np.max(coords, axis=0) - np.min(coords, axis=0))/voxel_size)

    a = ((coords - np.min(coords, axis=0)) // voxel_size).astype(int)
    non_empty_voxel_keys, inverse, nb_pts_per_voxel= np.unique(a, axis=0, return_inverse=True, return_counts=True)
    idx_pts_vox_sorted=np.argsort(inverse)
    voxel_grid={}
    grid_barycenter,grid_candidate_center=[],[]
    last_seen=0
    for idx, vox in enumerate(tqdm(non_empty_voxel_keys)):
        per_voxel = nb_pts_per_voxel[idx]
        num_pvs   = last_seen + per_voxel
        idxs      = idx_pts_vox_sorted[last_seen: num_pvs]
        voxel_grid[tuple(vox)] = points[idxs]
        gbc = np.mean(points[idxs], axis=0)
        grid_barycenter.append(gbc)
        #
        temp = voxel_grid[tuple(vox)]-np.mean(voxel_grid[tuple(vox)],axis=0)
        temp = np.linalg.norm(temp[:,0:2], axis=1).argmin()                                             
        grid_candidate_center.append(voxel_grid[tuple(vox)][temp])
        last_seen += per_voxel
    #
    grid_barycenter       = np.array(grid_barycenter)
    grid_candidate_center = np.array(grid_candidate_center)
    return {'barycenter': grid_barycenter, 'candidate': grid_candidate_center}

# %% ../V00/00_core.ipynb 4
class Lht_VariationalStrategy(VariationalStrategy):
    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True, jitter_val=None, max_tries=50):
        super().__init__( model, inducing_points, variational_distribution, learn_inducing_locations, jitter_val=jitter_val)
        self.register_buffer("updated_strategy", torch.tensor(True))
        self._register_load_state_dict_pre_hook(gpytorch.variational.variational_strategy._ensure_updated_strategy_flag_set)
        self.has_fantasy_strategy = True
        self.max_tries = max_tries
    @cached(name="cholesky_factor", ignore_args=True)
    def _cholesky_factor(self, induc_induc_covar):
        induc_induc_covar = to_dense(induc_induc_covar).type(_linalg_dtype_cholesky.value())
        #torch._assert(True, f"hello test in Lht_variational - {induc_induc_covar.dtype}")
        #induc_induc_covar.nan_to_num_(torch.randn(1).exp().item())
        #induc_induc_covar = induc_induc_covar.float()
        #torch._assert(True, f"hello test in Lht_variational - {induc_induc_covar.dtype}")
        max_tries = self.max_tries
        L = psd_safe_cholesky(induc_induc_covar, max_tries=max_tries)
        return linear_operator.operators.TriangularLinearOperator(L)

# %% ../V00/00_core.ipynb 6
class MaternFunction_old(torch.autograd.Function):
    @staticmethod
    def forward(ctx,dist,nu,sigma, beta,nugget):
        ctx.save_for_backward(dist,nu,sigma,beta,nugget)
        diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        SS = sigma ** 2
        g = nu.lgamma().exp()
        A = torch.as_tensor(2.).pow(1-nu).div(g).mul(sigma).mul(sigma)
        B = diff.div(beta).pow(nu)
        pa = diff.div(beta)
        K = kv(nu.item(), pa.cpu().detach().numpy())
        K = torch.from_numpy(K).float().to(dist.device)
        K = K.clip(max = torch.as_tensor(1e+10,dtype=torch.float32), min=1e-12)
        N = dist.where(dist > 1e-12, torch.as_tensor(nugget.pow(2)))
        ret = A.mul(B).mul(K) + N
        #ret = SS - ret       
        return ret
    
    @staticmethod
    def backward(ctx, grad_output):
        dist,nu,sigma,beta,nugget = ctx.saved_tensors
        diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        grad_nu = grad_sigma = grad_beta = grad_nugget = None
        # grad_nu
        g = nu.lgamma().exp()
        B = diff.div(beta).pow(nu)
        C = torch.as_tensor(2.).pow(1-nu).mul(B).div(g)
        
        a_1 = torch.as_tensor(2.).pow(1-nu).mul(B).mul(torch.log(diff.div(beta)))
        
        a_2 = torch.as_tensor(2.).pow(1-nu).mul(-torch.log(torch.as_tensor(2.0))).mul(B)
        A = a_1.add(a_2).mul(g)
        c_1 = torch.as_tensor(2.).pow(1-nu).mul(B)
        c_gamma_der = torch.digamma(nu).mul(g)
        C_der = A.subtract(c_1.mul(c_gamma_der)).div(g.pow(2))
        ##
        pa = diff.div(beta)
        K = kv(nu.item(), pa.cpu().detach().numpy())
        K = torch.from_numpy(K).float().to(dist.device)
        K = K.clip(max = torch.as_tensor(1e+12,dtype=torch.float32), min=1e-12)
        KVP = kvp(nu.item(), pa.cpu().detach().numpy(),n=1)
        KVP = torch.from_numpy(KVP).float().to(dist.device)
        KVP = KVP.clip(max = torch.as_tensor(1e+12,dtype=torch.float32), min=1e-12)
        ##
        tmp = C_der.mul(K) + C.mul(KVP)
        #grad_nu = -tmp
        grad_nu = sigma.pow(2).mul(tmp)
        # grad_sigma
        #grad_sigma = 2*sigma
        grad_sigma = torch.as_tensor(2.).pow(2-nu).mul(B).div(g).mul(sigma).mul(K)
        # grad_beta
        tmp = torch.as_tensor(2.).pow(1-nu).div(g).mul(sigma).mul(sigma)
        b_1 = (-nu).mul(diff.pow(nu)).div(beta.pow(nu+1)).mul(K)
        b_2 = B.mul(KVP).mul(-diff).div(beta.pow(2))
        grad_beta = b_1.add(b_2).mul(1*tmp)
        # grad_nugget
        grad_nugget = dist.where(dist > 1e-12, torch.as_tensor(2*nugget))
        return None, grad_nu, grad_sigma, grad_beta, grad_nugget

# %% ../V00/00_core.ipynb 7
class MaternFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx,dist,nu,sigma, beta,nugget):
        ctx.save_for_backward(dist,nu,sigma,beta,nugget)
        #
        diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        #diff = dist.where(dist <= 1e-12, torch.as_tensor(1.).to(dist.device))
        #
        SS = sigma ** 2
        g = nu.lgamma().exp()
        A = torch.as_tensor(2.).pow(1-nu).div(g).mul(sigma).mul(sigma)
        B = diff.div(beta).pow(nu)
        pa = diff.div(beta)
        K = kv(nu.item(), pa.cpu().detach().numpy())
        K = torch.from_numpy(K).float().to(dist.device)
        K = K.clip(max = torch.as_tensor(1e+10,dtype=torch.float32), min=1e-12)
        #
        ind = torch.zeros_like(diff)
        ind[diff<=1e-12]=1
        N = ind * nugget.pow(2)
        #N = dist.where(dist > 1e-12, torch.as_tensor(nugget.pow(2)))
        #N = dist.where(dist <=1e-12, )
        #
        ret = A.mul(B).mul(K) + N
        #ret = SS - ret       
        return ret
    
    @staticmethod
    def backward(ctx, grad_output):
        dist,nu,sigma,beta,nugget = ctx.saved_tensors
        diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        grad_nu = torch.as_tensor(0.).to(dist.device)
        grad_sigma = torch.as_tensor(0.).to(dist.device)
        grad_beta = torch.as_tensor(0.).to(dist.device)
        grad_nugget = torch.as_tensor(0.).to(dist.device)
        # grad_nu
        g = nu.lgamma().exp()
        B = diff.div(beta).pow(nu)
        C = torch.as_tensor(2.).pow(1-nu).mul(B).div(g)
        
        a_1 = torch.as_tensor(2.).pow(1-nu).mul(B).mul(torch.log(diff.div(beta)))
        
        a_2 = torch.as_tensor(2.).pow(1-nu).mul(-torch.log(torch.as_tensor(2.0))).mul(B)
        A = a_1.add(a_2).mul(g)
        c_1 = torch.as_tensor(2.).pow(1-nu).mul(B)
        c_gamma_der = torch.digamma(nu).mul(g)
        C_der = A.subtract(c_1.mul(c_gamma_der)).div(g.pow(2))
        ##
        pa = diff.div(beta)
        K = kv(nu.item(), pa.cpu().detach().numpy())
        K = torch.from_numpy(K).float().to(dist.device)
        K = K.clip(max = torch.as_tensor(1e+12,dtype=torch.float32), min=1e-12)
        KVP = kvp(nu.item(), pa.cpu().detach().numpy(),n=1)
        KVP = torch.from_numpy(KVP).float().to(dist.device)
        KVP = KVP.clip(max = torch.as_tensor(1e+12,dtype=torch.float32), min=1e-12)
        ##
        tmp = C_der.mul(K) + C.mul(KVP)
        #grad_nu = -tmp
        grad_nu = sigma.pow(2).mul(tmp)
        # grad_sigma
        #grad_sigma = 2*sigma
        grad_sigma = torch.as_tensor(2.).pow(2-nu).mul(B).div(g).mul(sigma).mul(K)
        # grad_beta
        tmp = torch.as_tensor(2.).pow(1-nu).div(g).mul(sigma).mul(sigma)
        b_1 = (-nu).mul(diff.pow(nu)).div(beta.pow(nu+1)).mul(K)
        b_2 = B.mul(KVP).mul(-diff).div(beta.pow(2))
        grad_beta = b_1.add(b_2).mul(1*tmp)
        # grad_nugget
        ind = torch.zeros_like(diff)
        ind[diff<=1e-12]=1.
        grad_nugget = ind * 2 * nugget
        #grad_nugget = dist.where(dist > 1e-12, torch.as_tensor(2*nugget))
        return None, grad_nu, grad_sigma, grad_beta, grad_nugget

# %% ../V00/00_core.ipynb 8
class AFirstKernel_old(gpytorch.kernels.Kernel):
    is_stationary = True
    has_lengthscale = True
    def __init__(self,param=[1,1,1,1],**kwargs):        
        super().__init__(**kwargs)
        p_nu, p_sigma, p_beta, p_nugget = param
        _nu_a, _nu_b = torch.randn(1).exp(), torch.randn(1).exp()
        _sigma_a, _sigma_b = torch.randn(1).exp(), torch.randn(1).exp()
        _beta_a, _beta_b = torch.randn(1).exp() , torch.randn(1).exp()
        _nugget_a, _nugget_b = torch.randn(1).exp(), torch.randn(1).exp()
        
        nu_constraint_a = gpytorch.constraints.Positive()
        nu_constraint_b = gpytorch.constraints.Positive()
        sigma_constraint_a = gpytorch.constraints.Positive()
        sigma_constraint_b = gpytorch.constraints.Positive()
        beta_constraint_a = gpytorch.constraints.GreaterThan(0.)
        beta_constraint_b = gpytorch.constraints.GreaterThan(0.)
        nugget_constraint_a = gpytorch.constraints.Positive()
        nugget_constraint_b = gpytorch.constraints.Positive()
        #
        para_nu_a = nn.Parameter(_nu_a*p_nu)
        para_nu_b = nn.Parameter(_nu_b*p_nu)
        para_sigma_a = nn.Parameter(_sigma_a*p_sigma)
        para_sigma_b = nn.Parameter(_sigma_b*p_sigma)
        para_beta_a = nn.Parameter(_beta_a*p_beta)
        para_beta_b = nn.Parameter(_beta_b*p_beta)
        para_nugget_a = nn.Parameter(_nugget_a*p_nugget)
        para_nugget_b = nn.Parameter(_nugget_b*p_nugget)
        
        self.register_parameter(name="raw_nu_a", parameter = para_nu_a,)
        self.register_parameter(name="raw_nu_b", parameter = para_nu_b,)
        self.register_parameter(name="raw_sigma_a", parameter = para_sigma_a,)
        self.register_parameter(name="raw_sigma_b", parameter = para_sigma_b,)
        self.register_parameter(name="raw_beta_a", parameter = para_beta_a,)
        self.register_parameter(name="raw_beta_b", parameter = para_beta_b,)
        self.register_parameter(name="raw_nugget_a", parameter = para_nugget_a,)
        self.register_parameter(name="raw_nugget_b", parameter = para_nugget_b,)
        
        self.register_constraint("raw_nu_a", nu_constraint_a)
        self.register_constraint("raw_nu_b", nu_constraint_b)
        self.register_constraint("raw_sigma_a", sigma_constraint_a)
        self.register_constraint("raw_sigma_b", sigma_constraint_b)
        self.register_constraint("raw_beta_a", beta_constraint_a)
        self.register_constraint("raw_beta_b", beta_constraint_b)
        self.register_constraint("raw_nugget_a", nugget_constraint_a)
        self.register_constraint("raw_nugget_b", nugget_constraint_b)
        
    def _set_nu_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_a)
        self.initialize(raw_nu_a=self.raw_nu_a_constraint.inverse_transform(value))
    def _set_nu_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_b)
        self.initialize(raw_nu_b=self.raw_nu_b_constraint.inverse_transform(value))
    def _set_sigma_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_a)
        self.initialize(raw_sigma_a=self.raw_sigma_a_constraint.inverse_transform(value))
    def _set_sigma_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_b)
        self.initialize(raw_sigma_b=self.raw_sigma_b_constraint.inverse_transform(value))
    def _set_beta_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_a)
        self.initialize(raw_beta_a=self.raw_beta_a_constraint.inverse_transform(value))
    def _set_beta_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_b)
        self.initialize(raw_beta_b=self.raw_beta_b_constraint.inverse_transform(value))
    def _set_nugget_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_a)
        self.initialize(raw_nugget_a=self.raw_nugget_a_constraint.inverse_transform(value))
    def _set_nugget_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_b)
        self.initialize(raw_nugget_b=self.raw_nugget_b_constraint.inverse_transform(value))
        
    
    @property
    def nu_a(self):
        return self.raw_nu_a_constraint.transform(self.raw_nu_a)
    @nu_a.setter
    def nu_a(self,value):
        return self._set_nu_a(value)
    @property
    def nu_b(self):
        return self.raw_nu_b_constraint.transform(self.raw_nu_b)
    @nu_b.setter
    def nu_b(self,value):
        return self._set_nu_b(value)
    @property
    def sigma_a(self):
        return self.raw_sigma_a_constraint.transform(self.raw_sigma_a)
    @sigma_a.setter
    def sigma_a(self,value):
        return self._set_sigma_a(value)
    @property
    def sigma_b(self):
        return self.raw_sigma_b_constraint.transform(self.raw_sigma_b)
    @sigma_b.setter
    def sigma_b(self,value):
        return self._set_sigma_b(value)
    @property
    def beta_a(self):
        return self.raw_beta_a_constraint.transform(self.raw_beta_a)
    @beta_a.setter
    def beta_a(self,value):
        return self._set_beta_a(value)
    @property
    def beta_b(self):
        return self.raw_beta_b_constraint.transform(self.raw_beta_b)
    @beta_b.setter
    def beta_b(self,value):
        return self._set_beta_b(value)
    @property
    def nugget_a(self):
        return self.raw_nugget_a_constraint.transform(self.raw_nugget_a)
    @nugget_a.setter
    def nugget_a(self, value):
        return self._set_nugget_a(value)
    @property
    def nugget_b(self):
        return self.raw_nugget_b_constraint.transform(self.raw_nugget_b)
    @nugget_b.setter
    def nugget_b(self, value):
        return self._set_nugget_b(value)
    
    def forward(self, x1, x2,diag=False, **params):
        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
        x1_ = (x1 - mean)
        x2_ = (x2 - mean)
        dist = self.covar_dist(x1_, x2_,diag=diag, **params)        
        #diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        
        
               
        nu_prior = gpytorch.priors.GammaPrior(self.nu_a, self.nu_b)
        sigma_prior = gpytorch.priors.GammaPrior(self.sigma_a, self.sigma_b)
        beta_prior = gpytorch.priors.GammaPrior(self.beta_a, self.beta_b)
        nugget_prior = gpytorch.priors.GammaPrior(self.nugget_a, self.nugget_b)
        nu = nu_prior.mean
        sigma = sigma_prior.mean
        beta = beta_prior.mean
        nugget = nugget_prior.mean
        
        ret = MaternFunction.apply(dist,nu,sigma,beta,nugget)       
        return ret

# %% ../V00/00_core.ipynb 9
class AFirstKernel(gpytorch.kernels.Kernel):
    is_stationary = True
    has_lengthscale = True
    def __init__(self,param=[1,1,1,1],**kwargs):        
        super().__init__(**kwargs)
        nugget = kwargs['nugget']
        self.nugget = torch.as_tensor(nugget, dtype=torch.float32)
        self.current_nugget = self.nugget
        p_nu, p_sigma, p_beta, p_nugget = param
        _nu_a, _nu_b = torch.randn(1).exp(), torch.randn(1).exp()
        _sigma_a, _sigma_b = torch.randn(1).exp(), torch.randn(1).exp()
        _beta_a, _beta_b = torch.randn(1).exp() , torch.randn(1).exp()
        _nugget_a, _nugget_b = torch.randn(1).exp(), torch.randn(1).exp()
        
        nu_constraint_a = gpytorch.constraints.Positive()
        nu_constraint_b = gpytorch.constraints.Positive()
        sigma_constraint_a = gpytorch.constraints.Positive()
        sigma_constraint_b = gpytorch.constraints.Positive()
        beta_constraint_a = gpytorch.constraints.GreaterThan(0.)
        beta_constraint_b = gpytorch.constraints.GreaterThan(0.)
        nugget_constraint_a = gpytorch.constraints.Positive()
        nugget_constraint_b = gpytorch.constraints.Positive()
        #
        para_nu_a = nn.Parameter(_nu_a*p_nu)
        para_nu_b = nn.Parameter(_nu_b*p_nu)
        para_sigma_a = nn.Parameter(_sigma_a*p_sigma)
        para_sigma_b = nn.Parameter(_sigma_b*p_sigma)
        para_beta_a = nn.Parameter(_beta_a*p_beta)
        para_beta_b = nn.Parameter(_beta_b*p_beta)
        para_nugget_a = nn.Parameter(_nugget_a*p_nugget)
        para_nugget_b = nn.Parameter(_nugget_b*p_nugget)
        
        self.register_parameter(name="raw_nu_a", parameter = para_nu_a,)
        self.register_parameter(name="raw_nu_b", parameter = para_nu_b,)
        self.register_parameter(name="raw_sigma_a", parameter = para_sigma_a,)
        self.register_parameter(name="raw_sigma_b", parameter = para_sigma_b,)
        self.register_parameter(name="raw_beta_a", parameter = para_beta_a,)
        self.register_parameter(name="raw_beta_b", parameter = para_beta_b,)
        self.register_parameter(name="raw_nugget_a", parameter = para_nugget_a,)
        self.register_parameter(name="raw_nugget_b", parameter = para_nugget_b,)
        
        self.register_constraint("raw_nu_a", nu_constraint_a)
        self.register_constraint("raw_nu_b", nu_constraint_b)
        self.register_constraint("raw_sigma_a", sigma_constraint_a)
        self.register_constraint("raw_sigma_b", sigma_constraint_b)
        self.register_constraint("raw_beta_a", beta_constraint_a)
        self.register_constraint("raw_beta_b", beta_constraint_b)
        self.register_constraint("raw_nugget_a", nugget_constraint_a)
        self.register_constraint("raw_nugget_b", nugget_constraint_b)
        
    def _set_nu_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_a)
        self.initialize(raw_nu_a=self.raw_nu_a_constraint.inverse_transform(value))
    def _set_nu_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_b)
        self.initialize(raw_nu_b=self.raw_nu_b_constraint.inverse_transform(value))
    def _set_sigma_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_a)
        self.initialize(raw_sigma_a=self.raw_sigma_a_constraint.inverse_transform(value))
    def _set_sigma_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_b)
        self.initialize(raw_sigma_b=self.raw_sigma_b_constraint.inverse_transform(value))
    def _set_beta_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_a)
        self.initialize(raw_beta_a=self.raw_beta_a_constraint.inverse_transform(value))
    def _set_beta_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_b)
        self.initialize(raw_beta_b=self.raw_beta_b_constraint.inverse_transform(value))
    def _set_nugget_a(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_a)
        self.initialize(raw_nugget_a=self.raw_nugget_a_constraint.inverse_transform(value))
    def _set_nugget_b(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_b)
        self.initialize(raw_nugget_b=self.raw_nugget_b_constraint.inverse_transform(value))
        
    
    @property
    def nu_a(self):
        return self.raw_nu_a_constraint.transform(self.raw_nu_a)
    @nu_a.setter
    def nu_a(self,value):
        return self._set_nu_a(value)
    @property
    def nu_b(self):
        return self.raw_nu_b_constraint.transform(self.raw_nu_b)
    @nu_b.setter
    def nu_b(self,value):
        return self._set_nu_b(value)
    @property
    def sigma_a(self):
        return self.raw_sigma_a_constraint.transform(self.raw_sigma_a)
    @sigma_a.setter
    def sigma_a(self,value):
        return self._set_sigma_a(value)
    @property
    def sigma_b(self):
        return self.raw_sigma_b_constraint.transform(self.raw_sigma_b)
    @sigma_b.setter
    def sigma_b(self,value):
        return self._set_sigma_b(value)
    @property
    def beta_a(self):
        return self.raw_beta_a_constraint.transform(self.raw_beta_a)
    @beta_a.setter
    def beta_a(self,value):
        return self._set_beta_a(value)
    @property
    def beta_b(self):
        return self.raw_beta_b_constraint.transform(self.raw_beta_b)
    @beta_b.setter
    def beta_b(self,value):
        return self._set_beta_b(value)
    @property
    def nugget_a(self):
        return self.raw_nugget_a_constraint.transform(self.raw_nugget_a)
    @nugget_a.setter
    def nugget_a(self, value):
        return self._set_nugget_a(value)
    @property
    def nugget_b(self):
        return self.raw_nugget_b_constraint.transform(self.raw_nugget_b)
    @nugget_b.setter
    def nugget_b(self, value):
        return self._set_nugget_b(value)
    
    def forward(self, x1, x2,diag=False, **params):
        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
        x1_ = (x1 - mean)
        x2_ = (x2 - mean)
        dist = self.covar_dist(x1_, x2_,diag=diag, **params)        
        #diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
        
        
               
        nu_prior = gpytorch.priors.GammaPrior(self.nu_a, self.nu_b)
        sigma_prior = gpytorch.priors.GammaPrior(self.sigma_a, self.sigma_b)
        beta_prior = gpytorch.priors.GammaPrior(self.beta_a, self.beta_b)
        nugget_prior = gpytorch.priors.GammaPrior(self.nugget_a, self.nugget_b)
        nu = nu_prior.mean
        sigma = sigma_prior.mean
        beta = beta_prior.mean
        nugget = nugget_prior.mean + self.nugget
        #
        self.current_nu     = nu
        self.current_sigma  = sigma
        self.current_beta   = beta
        self.current_nugget = nugget
        
        ret = MaternFunction.apply(dist,nu,sigma,beta,nugget)       
        return ret

# %% ../V00/00_core.ipynb 10
class Kernel_HalfNorm(gpytorch.kernels.Kernel):
    is_stationary = True
    has_lengthscale = True
    def __init__(self,**kwargs):        
        super().__init__(**kwargs)
        nugget = kwargs['nugget']
        self.nugget = torch.as_tensor(nugget, dtype=torch.float32)
        self.current_nugget = self.nugget
        _nu_scale = torch.randn(1).exp()
        _sigma_scale = torch.randn(1).exp()
        _beta_scale = torch.randn(1).exp()
        _nugget_scale = torch.randn(1).exp()
        
        nu_constraint = gpytorch.constraints.Positive()
        sigma_constraint = gpytorch.constraints.Positive()
        beta_constraint = gpytorch.constraints.GreaterThan(0.)
        nugget_constraint = gpytorch.constraints.Positive()

        #
        para_nu = nn.Parameter(_nu_scale)
        para_sigma = nn.Parameter(_sigma_scale)
        para_beta = nn.Parameter(_beta_scale)
        para_nugget = nn.Parameter(_nugget_scale)
        
        self.register_parameter(name="raw_nu_scale", parameter = para_nu,)
        self.register_parameter(name="raw_sigma_scale", parameter = para_sigma,)
        self.register_parameter(name="raw_beta_scale", parameter = para_beta,)
        self.register_parameter(name="raw_nugget_scale", parameter = para_nugget,)
        
        self.register_constraint("raw_nu_scale", nu_constraint)
        self.register_constraint("raw_sigma_scale", sigma_constraint)
        self.register_constraint("raw_beta_scale", beta_constraint)
        self.register_constraint("raw_nugget_scale", nugget_constraint)
        
    def _set_nu_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_scale)
        self.initialize(raw_nu_scale=self.raw_nu_scale_constraint.inverse_transform(value))
   
    def _set_sigma_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_scale)
        self.initialize(raw_sigma_scale=self.raw_sigma_scale_constraint.inverse_transform(value))
   
    def _set_beta_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_scale)
        self.initialize(raw_beta_scale=self.raw_beta_scale_constraint.inverse_transform(value))
    
    def _set_nugget_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_scale)
        self.initialize(raw_nugget_scale=self.raw_nugget_scale_constraint.inverse_transform(value))
    
        
    
    @property
    def nu_scale(self):
        return self.raw_nu_scale_constraint.transform(self.raw_nu_scale)
    @nu_scale.setter
    def nu_scale(self,value):
        return self._set_nu_scale(value)
    
    @property
    def sigma_scale(self):
        return self.raw_sigma_scale_constraint.transform(self.raw_sigma_scale)
    @sigma_scale.setter
    def sigma_scale(self,value):
        return self._set_sigma_scale(value)
   
    @property
    def beta_scale(self):
        return self.raw_beta_scale_constraint.transform(self.raw_beta_scale)
    @beta_scale.setter
    def beta_scale(self,value):
        return self._set_beta_scale(value)
    
    @property
    def nugget_scale(self):
        return self.raw_nugget_scale_constraint.transform(self.raw_nugget_scale)
    @nugget_scale.setter
    def nugget_scale(self, value):
        return self._set_nugget_scale(value)
   
    
    def forward(self, x1, x2,diag=False, **params):
        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
        x1_ = (x1 - mean)
        x2_ = (x2 - mean)
        dist = self.covar_dist(x1_, x2_,diag=diag, **params)        
        #diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
                      
        nu_prior = gpytorch.priors.HalfNormalPrior(self.nu_scale)
        sigma_prior = gpytorch.priors.HalfNormalPrior(self.sigma_scale)
        beta_prior = gpytorch.priors.HalfNormalPrior(self.beta_scale)
        nugget_prior = gpytorch.priors.HalfNormalPrior(self.nugget_scale)
        nu = nu_prior.mean
        sigma = sigma_prior.mean
        beta = beta_prior.mean
        nugget = nugget_prior.mean + self.nugget
        #
        self.current_nu     = nu
        self.current_sigma  = sigma
        self.current_beta   = beta
        self.current_nugget = nugget
        
        ret = MaternFunction.apply(dist,nu,sigma,beta,nugget)       
        return ret

# %% ../V00/00_core.ipynb 11
class Kernel_HalfNorm_old(gpytorch.kernels.Kernel):
    is_stationary = True
    has_lengthscale = True
    def __init__(self,**kwargs):        
        super().__init__(**kwargs)
        _nu_scale = torch.randn(1).exp()
        _sigma_scale = torch.randn(1).exp()
        _beta_scale = torch.randn(1).exp()
        _nugget_scale = torch.randn(1).exp()
        
        nu_constraint = gpytorch.constraints.Positive()
        sigma_constraint = gpytorch.constraints.Positive()
        beta_constraint = gpytorch.constraints.GreaterThan(0.)
        nugget_constraint = gpytorch.constraints.Positive()

        #
        para_nu = nn.Parameter(_nu_scale)
        para_sigma = nn.Parameter(_sigma_scale)
        para_beta = nn.Parameter(_beta_scale)
        para_nugget = nn.Parameter(_nugget_scale)
        
        self.register_parameter(name="raw_nu_scale", parameter = para_nu,)
        self.register_parameter(name="raw_sigma_scale", parameter = para_sigma,)
        self.register_parameter(name="raw_beta_scale", parameter = para_beta,)
        self.register_parameter(name="raw_nugget_scale", parameter = para_nugget,)
        
        self.register_constraint("raw_nu_scale", nu_constraint)
        self.register_constraint("raw_sigma_scale", sigma_constraint)
        self.register_constraint("raw_beta_scale", beta_constraint)
        self.register_constraint("raw_nugget_scale", nugget_constraint)
        
    def _set_nu_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nu_scale)
        self.initialize(raw_nu_scale=self.raw_nu_scale_constraint.inverse_transform(value))
   
    def _set_sigma_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.sigma_scale)
        self.initialize(raw_sigma_scale=self.raw_sigma_scale_constraint.inverse_transform(value))
   
    def _set_beta_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.beta_scale)
        self.initialize(raw_beta_scale=self.raw_beta_scale_constraint.inverse_transform(value))
    
    def _set_nugget_scale(self, value):
        if not torch.is_tensor(value):
            value = torch.as_tensor(value).to(self.nugget_scale)
        self.initialize(raw_nugget_scale=self.raw_nugget_scale_constraint.inverse_transform(value))
    
        
    
    @property
    def nu_scale(self):
        return self.raw_nu_scale_constraint.transform(self.raw_nu_scale)
    @nu_scale.setter
    def nu_scale(self,value):
        return self._set_nu_scale(value)
    
    @property
    def sigma_scale(self):
        return self.raw_sigma_scale_constraint.transform(self.raw_sigma_scale)
    @sigma_scale.setter
    def sigma_scale(self,value):
        return self._set_sigma_scale(value)
   
    @property
    def beta_scale(self):
        return self.raw_beta_scale_constraint.transform(self.raw_beta_scale)
    @beta_scale.setter
    def beta_scale(self,value):
        return self._set_beta_scale(value)
    
    @property
    def nugget_scale(self):
        return self.raw_nugget_scale_constraint.transform(self.raw_nugget_scale)
    @nugget_scale.setter
    def nugget_scale(self, value):
        return self._set_nugget_scale(value)
   
    
    def forward(self, x1, x2,diag=False, **params):
        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]
        x1_ = (x1 - mean)
        x2_ = (x2 - mean)
        dist = self.covar_dist(x1_, x2_,diag=diag, **params)        
        #diff = dist.where(dist > 1e-12, torch.as_tensor(1e-12).to(dist.device))
                      
        nu_prior = gpytorch.priors.HalfNormalPrior(self.nu_scale)
        sigma_prior = gpytorch.priors.HalfNormalPrior(self.sigma_scale)
        beta_prior = gpytorch.priors.HalfNormalPrior(self.beta_scale)
        nugget_prior = gpytorch.priors.HalfNormalPrior(self.nugget_scale)
        nu = nu_prior.mean
        sigma = sigma_prior.mean
        beta = beta_prior.mean
        nugget = nugget_prior.mean
        
        ret = MaternFunction.apply(dist,nu,sigma,beta,nugget)       
        return ret

# %% ../V00/00_core.ipynb 21
class StandardApproximateGP_mat(ApproximateGP):
    def __init__(self, inducing_points, nu):
        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))
        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True,jitter_val=1e-1)
        super(StandardApproximateGP_mat, self).__init__(variational_strategy)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=nu, ard_num_dims=2))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# %% ../V00/00_core.ipynb 22
class GP_AFirst(ApproximateGP):
    def __init__(self, inducing_points, nu=1, max_tries=50, prior='gamma',nugget=0.25):
        if nu==1:
            variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))
        elif nu==2:
            variational_distribution = NaturalVariationalDistribution(inducing_points.size(0))
        else:
            variational_distribution = TrilNaturalVariationalDistribution(inducing_points.size(0))
        variational_strategy = Lht_VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True,jitter_val=1e-1, max_tries=max_tries)
        super().__init__(variational_strategy)
        self.prior = prior
        self.mean_module = gpytorch.means.ConstantMean()
        if prior=='gamma':
            self.covar_module = gpytorch.kernels.ScaleKernel(AFirstKernel(nugget=nugget))
        else:
            self.covar_module = gpytorch.kernels.ScaleKernel(Kernel_HalfNorm(nugget=nugget))

    def forward(self, x):
        mean_x = self.mean_module(x) 
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# %% ../V00/00_core.ipynb 23
def grid_dataset(points, voxel_size):
    "Define a function that takes as input an array of points, and a voxel size expressed in meters"
    "points: it is from a LiDAR data; voxel_size: the length of edge"
    coords = points[:,0:2]
    nb_vox=np.ceil((np.max(coords, axis=0) - np.min(coords, axis=0))/voxel_size)

    a = ((coords - np.min(coords, axis=0)) // voxel_size).astype(int)
    non_empty_voxel_keys, inverse, nb_pts_per_voxel= np.unique(a, axis=0, return_inverse=True, return_counts=True)
    idx_pts_vox_sorted=np.argsort(inverse)
    voxel_grid={}
    grid_barycenter,grid_candidate_center=[],[]
    last_seen=0
    for idx, vox in enumerate(tqdm.tqdm(non_empty_voxel_keys)):
        per_voxel = nb_pts_per_voxel[idx]
        num_pvs   = last_seen + per_voxel
        idxs      = idx_pts_vox_sorted[last_seen: num_pvs]
        voxel_grid[tuple(vox)] = points[idxs]
        last_seen += per_voxel
    
    return voxel_grid

def findNeighbors_old(r,c,ur,uc):
    ns = []
    if r-1 >=0:
        ns.append((r-1,c))
    if r+1 <= ur:
        ns.append((r+1,c))
    if c-1 >=0:
        ns.append((r,c-1))
    if c+1 <= uc:
        ns.append((r,c+1))
    return ns  

def findNeighbors(r,c,ur,uc):
    ns = []
    if r-1 >=0:
        ns.append((r-1,c))
    if r+1 <= ur:
        ns.append((r+1,c))
    if c-1 >=0:
        ns.append((r,c-1))
    if c+1 <= uc:
        ns.append((r,c+1))
    if r-1 >=0 and c-1 >=0:
        ns.append((r-1,c-1))
    if r-1 >=0 and c+1 <=uc:
        ns.append((r-1,c+1))
    if r+1 <=ur and c-1 >=0:
        ns.append((r+1,c-1))
    if r+1 <=ur and c+1 <=uc:
        ns.append((r+1,c+1))
    return ns  
