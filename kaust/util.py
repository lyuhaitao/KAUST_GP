# AUTOGENERATED! DO NOT EDIT! File to edit: ../V00/01_util.ipynb.

# %% auto 0
__all__ = ['showCI_NSBN', 'showCI_NSBN_old', 'CI_NSBN', 'df_to_dataload', 'show_mae_acc', 'train_standardGPMatern',
           'train_LhtMatern_old', 'train_LhtMatern', 'train_LhtMatern_step_old', 'train_LhtMatern_step', 'test_records',
           'lht_test', 'lht_train_one_epoch_old', 'lht_train_one_epoch']

# %% ../V00/01_util.ipynb 2
import kaust.core as lhtcore
from .core import *
import torch
import tqdm
import gpytorch
import numpy as np
import scipy
import pandas as pd
from torch.utils.data import Dataset, DataLoader, TensorDataset
import os

# %% ../V00/01_util.ipynb 6
def showCI_NSBN(model):
    cis_a1_train_1 = {}
    ker = model.covar_module.base_kernel
    if model.prior=='gamma':
        nu_a,nu_b = ker.nu_a, ker.nu_b
        cis_a1_train_1['nu'] = scipy.stats.gamma.interval(confidence=0.95, a=nu_a.item(), scale= 1/nu_b.item(), loc=0)
        sigma_a, sigma_b = ker.sigma_a, ker.sigma_b
        cis_a1_train_1['sigma'] = scipy.stats.gamma.interval(confidence=0.95, a=sigma_a.item(), scale= 1/sigma_b.item())
        beta_a, beta_b = ker.beta_a, ker.beta_b
        cis_a1_train_1['beta'] = scipy.stats.gamma.interval(confidence=0.95, a=beta_a.item(), scale= 1/beta_b.item())
        nugget_a, nugget_b = ker.nugget_a, ker.nugget_b
        a = scipy.stats.gamma.interval(confidence=0.95, a=nugget_a.item(), scale=1/nugget_b.item())
        cis_a1_train_1['nugget'] = [i+ker.nugget.item() for i in a]
    else:
        #print('halfnorm')
        nu_scale, sigma_scale, beta_scale, nugget_scale = ker.nu_scale,ker.sigma_scale, ker.beta_scale, ker.nugget_scale
        cis_a1_train_1['nu'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=nu_scale.item(), loc=0)
        cis_a1_train_1['sigma'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=sigma_scale.item(), loc=0)
        cis_a1_train_1['beta'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=beta_scale.item(), loc=0)
        a = scipy.stats.halfnorm.interval(confidence=0.95,scale=nugget_scale.item(), loc=0)
        cis_a1_train_1['nugget'] =  [i+ker.nugget.item() for i in a]
        
    return pd.DataFrame(cis_a1_train_1)


def showCI_NSBN_old(model):
    cis_a1_train_1 = {}
    ker = model.covar_module.base_kernel
    if model.prior=='gamma':
        nu_a,nu_b = ker.nu_a, ker.nu_b
        cis_a1_train_1['nu'] = scipy.stats.gamma.interval(confidence=0.95, a=nu_a.item(), scale= 1/nu_b.item(), loc=0)
        sigma_a, sigma_b = ker.sigma_a, ker.sigma_b
        cis_a1_train_1['sigma'] = scipy.stats.gamma.interval(confidence=0.95, a=sigma_a.item(), scale= 1/sigma_b.item())
        beta_a, beta_b = ker.beta_a, ker.beta_b
        cis_a1_train_1['beta'] = scipy.stats.gamma.interval(confidence=0.95, a=beta_a.item(), scale= 1/beta_b.item())
        nugget_a, nugget_b = ker.nugget_a, ker.nugget_b
        cis_a1_train_1['nugget'] = scipy.stats.gamma.interval(confidence=0.95, a=nugget_a.item(), scale=1/nugget_b.item())
    else:
        #print('halfnorm')
        nu_scale, sigma_scale, beta_scale, nugget_scale = ker.nu_scale,ker.sigma_scale, ker.beta_scale, ker.nugget_scale
        cis_a1_train_1['nu'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=nu_scale.item(), loc=0)
        cis_a1_train_1['sigma'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=sigma_scale.item(), loc=0)
        cis_a1_train_1['beta'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=beta_scale.item(), loc=0)
        cis_a1_train_1['nugget'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=nugget_scale.item(), loc=0)
        
    return pd.DataFrame(cis_a1_train_1)

def CI_NSBN(model):
    cis_a1_train_1 = {}
    ker = model.covar_module.base_kernel
    if model.prior=='gamma':
        nu_a,nu_b = ker.nu_a, ker.nu_b
        cis_a1_train_1['nu'] = scipy.stats.gamma.interval(confidence=0.95, a=nu_a.item(), scale= 1/nu_b.item(), loc=0)
        sigma_a, sigma_b = ker.sigma_a, ker.sigma_b
        cis_a1_train_1['sigma'] = scipy.stats.gamma.interval(confidence=0.95, a=sigma_a.item(), scale= 1/sigma_b.item())
        beta_a, beta_b = ker.beta_a, ker.beta_b
        cis_a1_train_1['beta'] = scipy.stats.gamma.interval(confidence=0.95, a=beta_a.item(), scale= 1/beta_b.item())
        nugget_a, nugget_b = ker.nugget_a, ker.nugget_b
        cis_a1_train_1['nugget'] = scipy.stats.gamma.interval(confidence=0.95, a=nugget_a.item(), scale=1/nugget_b.item())
    else:
        #print('halfnorm')
        nu_scale, sigma_scale, beta_scale, nugget_scale = ker.nu_scale,ker.sigma_scale, ker.beta_scale, ker.nugget_scale
        cis_a1_train_1['nu'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=nu_scale.item(), loc=0)
        cis_a1_train_1['sigma'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=sigma_scale.item(), loc=0)
        cis_a1_train_1['beta'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=beta_scale.item(), loc=0)
        cis_a1_train_1['nugget'] = scipy.stats.halfnorm.interval(confidence=0.95,scale=nugget_scale.item(), loc=0)
        
    return cis_a1_train_1

def df_to_dataload(df,batch_size=200, cuda=False):
    points = df.to_numpy()
    train_x = points[:,0:2]
    train_y = points[:,2]
    train_x = torch.from_numpy(train_x).float()
    train_y = torch.from_numpy(train_y).float()
    if cuda:
        train_x, train_y = train_x.cuda(), train_y.cuda()
    train_dataset = TensorDataset(train_x, train_y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    return train_loader

# %% ../V00/01_util.ipynb 7
def show_mae_acc(df, model):
    points = df.to_numpy()
    X = points[:,0:2]
    Y = points[:,2]
    model.eval()
    tx = torch.from_numpy(X).float()
    yy = torch.from_numpy(Y).float()
    preds = model(tx)
    means = preds.mean.cpu()
    error = torch.mean(torch.abs(means - yy.cpu()))
    l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
    ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
    LB,UB = ci
    a = np.logical_and(Y>=LB, Y<=UB)
    acc = Y[a].shape[0] / len(Y)
    return error.item(), acc

# %% ../V00/01_util.ipynb 8
def train_standardGPMatern(df, varogram_dist, inducing_points, cuda=True, nu=2.5, lr=0.01, batch_size=200, num_epoch=1, folder=None):
    if folder == None:
        torch._assert(False, "need a folder to save the parameters of models")
    points = df.to_numpy()
    voxel_grid = grid_dataset(points,varogram_dist)

    keys = np.array(list(voxel_grid.keys()))
    ur, uc = keys.max(axis=0)
    cv_ds = []
    for k,v in voxel_grid.items():
        ret = {'test':None, 'train':None}
        ret['test'] = v
        r,c = k
        neighbours = findNeighbors(r,c,ur=ur,uc=uc)
        neighbours += [k]
        train = []
        for i,j in voxel_grid.items():
            if i in neighbours:
                continue
            train.append(j.tolist())
        ret['train'] = np.vstack(train)
        cv_ds.append(ret)
    #
    records=[]
    model = StandardApproximateGP_mat(inducing_points=inducing_points, nu=nu)
    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    if cuda:
        model = model.cuda()
        likelihood = likelihood.cuda()
    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=lr)
    inds = len(cv_ds)
    cv_iter = tqdm.notebook.tqdm(range(num_epoch*inds), desc='cross-validation')
    for i in cv_iter:
        points = cv_ds[i%inds]['test']
        test_x = points[:,0:2]
        test_y = points[:,2]
        test_x = torch.from_numpy(test_x).float()
        test_y = torch.from_numpy(test_y).float()
        if cuda:
            test_x = test_x.cuda()
            test_y = test_y.cuda()
        #
        points = cv_ds[i%inds]['train']
        train_x = points[:,0:2]
        train_y = points[:,2]
        train_x = torch.from_numpy(train_x).float()
        train_y = torch.from_numpy(train_y).float()
        if cuda:
            train_x, train_y = train_x.cuda(), train_y.cuda()
        train_dataset = TensorDataset(train_x, train_y)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        model.train()
        likelihood.train()
        mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
        for x_batch, y_batch in minibatch_iter:
            optimizer.zero_grad()
            output = model(x_batch)
            loss = -mll(output, y_batch)
            minibatch_iter.set_postfix(loss=loss.item())
            loss.backward()
            optimizer.step()
        # Test
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
        model.eval()
        likelihood.eval()
        means = torch.tensor([0.])
        with torch.no_grad():
            test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)
            for x_batch, y_batch in test_iter:
                preds = model(x_batch)
                means = torch.cat([means, preds.mean.cpu()])
            #
            means = means[1:]
            error = torch.mean(torch.abs(means - test_y.cpu()))

        with torch.no_grad():
            preds = model(test_x)
            l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
            ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
            LB,UB = ci
            LB = torch.as_tensor(LB,dtype=torch.float32, device=torch.device('cuda'))
            UB = torch.as_tensor(UB,dtype=torch.float32, device=torch.device('cuda'))
            a = torch.logical_and(test_y>=LB, test_y<=UB)
            acc = test_y[a].numel() / test_y.numel()
        #
        print(f"Test, MAE: {error.item()} ,  ACC: {acc}, train:{train_y.numel()}, test:{test_y.numel()}")
         #
        param_fp = os.path.join(folder, f'{fid}.pth')
        torch.save(model.state_dict(), param_fp) 
        ma = {'mae':error.item(), 'acc':acc, 'param':param_fp}
        records.append(ma)
    return records

# %% ../V00/01_util.ipynb 9
def train_LhtMatern_old(df, varogram_dist, inducing_points,prior='gamma', cuda=False, lr=0.01, batch_size=200, num_epoch=1, show_ci=True, folder=None):
    ''' don't support to use CUDA
    parameters:
    returen
        records: List[dict] training results
    '''
    if folder == None:
        torch._assert(False, "need a folder to save the parameters of models")
    points = df.to_numpy()
    voxel_grid = grid_dataset(points,varogram_dist)
    keys = np.array(list(voxel_grid.keys()))
    ur, uc = keys.max(axis=0)
    cv_ds = []
    for k,v in voxel_grid.items():
        ret = {'test':None, 'train':None}
        ret['test'] = v
        r,c = k
        neighbours = findNeighbors(r,c,ur=ur,uc=uc)
        neighbours += [k]
        train = []
        for i,j in voxel_grid.items():
            if i in neighbours:
                continue
            train.append(j.tolist())
        ret['train'] = np.vstack(train)
        cv_ds.append(ret)
    #
    records=[]
    model = GP_AFirst(inducing_points,nu=1, max_tries=200,prior=prior)
    likelihood = gpytorch.likelihoods.GaussianLikelihood()  
    if cuda:
        model = model.cuda()
        likelihood = likelihood.cuda()
    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=lr)
    if show_ci:
        print(showCI_NSBN(model))
    inds = len(cv_ds)
    cv_iter = tqdm.notebook.tqdm(range(num_epoch*inds), desc='cross-validation')

    for fid, i in enumerate(cv_iter):
        points = cv_ds[i%inds]['test']
        test_x = points[:,0:2]
        test_y = points[:,2]
        test_x = torch.from_numpy(test_x).float()
        test_y = torch.from_numpy(test_y).float()
        #
        points = cv_ds[i%inds]['train']
        train_x = points[:,0:2]
        train_y = points[:,2]
        train_x = torch.from_numpy(train_x).float()
        train_y = torch.from_numpy(train_y).float()
        if cuda:
            train_x, train_y = train_x.cuda(), train_y.cuda()
        train_dataset = TensorDataset(train_x, train_y)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        model.train()
        likelihood.train()
        mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
        for x_batch, y_batch in minibatch_iter:
            # Perform Adam step to optimize hyperparameters
            optimizer.zero_grad()
            output = model(x_batch)
            loss = -mll(output, y_batch)
            loss.backward()
            optimizer.step()
            #
            if model.prior == 'gamma':
                nu_a = model.covar_module.base_kernel.nu_a
                sigma_a = model.covar_module.base_kernel.sigma_a
                beta_a = model.covar_module.base_kernel.beta_a
                nugget_a = model.covar_module.base_kernel.nugget_a
                minibatch_iter.set_postfix(loss=loss.item(), nu_a=nu_a.item(), sigma_a=sigma_a.item(),beta_a=beta_a.item(),nugget_a=nugget_a.item())
                torch._assert(not torch.isnan(nu_a),"nu_a is nan")
            else:
                nu_scale = model.covar_module.base_kernel.nu_scale
                sigma_scale = model.covar_module.base_kernel.sigma_scale
                beta_scale = model.covar_module.base_kernel.beta_scale
                nugget_scale = model.covar_module.base_kernel.nugget_scale
                minibatch_iter.set_postfix(loss=loss.item(), nu=nu_scale.item(), sigma=sigma_scale.item(),beta=beta_scale.item(),nugget=nugget_scale.item())
        #
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
        model.eval()
        likelihood.eval()
        means = torch.tensor([0.])
        with torch.no_grad():
            test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)
            for x_batch, y_batch in test_iter:
                preds = model(x_batch)
                means = torch.cat([means, preds.mean.cpu()])
            #
            means = means[1:]
            error = torch.mean(torch.abs(means - test_y.cpu()))

        with torch.no_grad():
            preds = model(test_x)
            l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
            ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
            LB,UB = ci
            LB = torch.as_tensor(LB,dtype=torch.float32, device=torch.device('cpu'))
            UB = torch.as_tensor(UB,dtype=torch.float32, device=torch.device('cpu'))
            a = torch.logical_and(test_y>=LB, test_y<=UB)
            acc = test_y[a].numel() / test_y.numel()
        #
        print(f"Test, MAE: {error.item()} ,  ACC: {acc}, train:{train_y.numel()}, test:{test_y.numel()}")
        if show_ci:
            print(showCI_NSBN(model))
        #
        param_fp = os.path.join(folder, f'{fid}.pth')
        torch.save(model.state_dict(), param_fp) 
        ma = {'mae':error.item(), 'acc':acc, 'param':param_fp}
        records.append(ma)
    return records

# %% ../V00/01_util.ipynb 10
def train_LhtMatern(df, varogram_dist, inducing_points,prior='gamma', cuda=False, lr=0.01, batch_size=200, num_epoch=1, show_ci=True, folder=None,nugget=0.3):
    ''' don't support to use CUDA
    parameters:
    returen
        records: List[dict] training results
    '''
    best_mae = 3
    
    if folder == None:
        torch._assert(False, "need a folder to save the parameters of models")
    points = df.to_numpy()
    voxel_grid = grid_dataset(points,varogram_dist)
    keys = np.array(list(voxel_grid.keys()))
    ur, uc = keys.max(axis=0)
    cv_ds = []
    for k,v in voxel_grid.items():
        ret = {'test':None, 'train':None}
        ret['test'] = v
        r,c = k
        neighbours = findNeighbors(r,c,ur=ur,uc=uc)
        neighbours += [k]
        train = []
        for i,j in voxel_grid.items():
            if i in neighbours:
                continue
            train.append(j.tolist())
        ret['train'] = np.vstack(train)
        cv_ds.append(ret)
    #
    records=[]
    model = lhtcore.GP_AFirst(inducing_points,nu=1, max_tries=200,prior=prior,nugget=nugget)

    likelihood = gpytorch.likelihoods.GaussianLikelihood()  
    if cuda:
        model = model.cuda()
        likelihood = likelihood.cuda()
    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=lr)

    if show_ci:
        print(showCI_NSBN(model))
    inds = len(cv_ds)
    cv_iter = tqdm.notebook.tqdm(range(num_epoch*inds), desc='cross-validation')

    for fid, i in enumerate(cv_iter):
        points = cv_ds[i%inds]['test']
        test_x = points[:,0:2]
        test_y = points[:,2]
        test_x = torch.from_numpy(test_x).float()
        test_y = torch.from_numpy(test_y).float()
        #
        points = cv_ds[i%inds]['train']
        train_x = points[:,0:2]
        train_y = points[:,2]
        train_x = torch.from_numpy(train_x).float()
        train_y = torch.from_numpy(train_y).float()
        if cuda:
            train_x, train_y = train_x.cuda(), train_y.cuda()
        train_dataset = TensorDataset(train_x, train_y)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        model.train()
        likelihood.train()
        mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
        for x_batch, y_batch in minibatch_iter:
            # Perform Adam step to optimize hyperparameters
            optimizer.zero_grad()
            output = model(x_batch)
            loss = -mll(output, y_batch)
            loss.backward()
            optimizer.step()
            #
            
            nu = model.covar_module.base_kernel.current_nu
            sigma = model.covar_module.base_kernel.current_sigma
            beta = model.covar_module.base_kernel.current_beta
            nugget = model.covar_module.base_kernel.current_nugget
            minibatch_iter.set_postfix(loss=loss.item(), nu=nu.item(), sigma=sigma.item(),beta=beta.item(),nugget=nugget.item())
            torch._assert(not torch.isnan(nu),"nu is nan")
           
        #
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
        model.eval()
        likelihood.eval()
        means = torch.tensor([0.])
        with torch.no_grad():
            test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)
            for x_batch, y_batch in test_iter:
                preds = model(x_batch)
                means = torch.cat([means, preds.mean.cpu()])
            #
            means = means[1:]
            error = torch.mean(torch.abs(means - test_y.cpu()))

        with torch.no_grad():
            preds = model(test_x)
            l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
            ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
            LB,UB = ci
            LB = torch.as_tensor(LB,dtype=torch.float32, device=torch.device('cpu'))
            UB = torch.as_tensor(UB,dtype=torch.float32, device=torch.device('cpu'))
            a = torch.logical_and(test_y>=LB, test_y<=UB)
            acc = test_y[a].numel() / test_y.numel()
        #
        print(f"Test, MAE: {error.item()} ,  ACC: {acc}, train:{train_y.numel()}, test:{test_y.numel()}")
        if show_ci:
            print(showCI_NSBN(model))
        #
        param_fp = os.path.join(folder, f'{fid}.pth')
        torch.save(model.state_dict(), param_fp) 
        ma = {'mae':error.item(), 'acc':acc, 'param':param_fp}
        records.append(ma)
    return records

# %% ../V00/01_util.ipynb 11
def train_LhtMatern_step_old(df, varogram_dist, inducing_points,prior='gamma', cuda=False, lr=0.01, base_lr=0.0001, lr_step=50,
                         batch_size=200, num_epoch=1, show_ci=True, folder=None,nugget=0.3):
    ''' don't support to use CUDA
    parameters:
    returen
        records: List[dict] training results
    '''
    best_mae = 3
    
    if folder == None:
        torch._assert(False, "need a folder to save the parameters of models")
    points = df.to_numpy()
    voxel_grid = grid_dataset(points,varogram_dist)
    keys = np.array(list(voxel_grid.keys()))
    ur, uc = keys.max(axis=0)
    cv_ds = []
    for k,v in voxel_grid.items():
        ret = {'test':None, 'train':None}
        ret['test'] = v
        r,c = k
        neighbours = findNeighbors(r,c,ur=ur,uc=uc)
        neighbours += [k]
        train = []
        for i,j in voxel_grid.items():
            if i in neighbours:
                continue
            train.append(j.tolist())
        ret['train'] = np.vstack(train)
        cv_ds.append(ret)
    #
    records=[]
    model = lhtcore.GP_AFirst(inducing_points,nu=1, max_tries=200,prior=prior,nugget=nugget)

    likelihood = gpytorch.likelihoods.GaussianLikelihood()  
    if cuda:
        model = model.cuda()
        likelihood = likelihood.cuda()
    #optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=lr)
    params = list(model.parameters()) + list(likelihood.parameters())
    optimizer = torch.optim.Adam(params, lr = lr, weight_decay=0.01 )
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=lr_step, eta_min=base_lr)
                                                                                   
    if show_ci:
        print(showCI_NSBN(model))
    inds = len(cv_ds)
    a = min(1000,num_epoch*inds)
    cv_iter = tqdm.notebook.tqdm(range(a), desc='cross-validation')

    for fid, i in enumerate(cv_iter):
        points = cv_ds[i%inds]['test']
        test_x = points[:,0:2]
        test_y = points[:,2]
        test_x = torch.from_numpy(test_x).float()
        test_y = torch.from_numpy(test_y).float()
        #
        points = cv_ds[i%inds]['train']
        train_x = points[:,0:2]
        train_y = points[:,2]
        train_x = torch.from_numpy(train_x).float()
        train_y = torch.from_numpy(train_y).float()
        if cuda:
            train_x, train_y = train_x.cuda(), train_y.cuda()
        train_dataset = TensorDataset(train_x, train_y)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        model.train()
        likelihood.train()
        mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
        for x_batch, y_batch in minibatch_iter:
            # Perform Adam step to optimize hyperparameters
            optimizer.zero_grad()
            output = model(x_batch)
            loss = -mll(output, y_batch)
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            #
            lrr = optimizer.param_groups[0]['lr']
            nu = model.covar_module.base_kernel.current_nu
            sigma = model.covar_module.base_kernel.current_sigma
            beta = model.covar_module.base_kernel.current_beta
            nugget = model.covar_module.base_kernel.current_nugget
            minibatch_iter.set_postfix(loss=loss.item(), nu=nu.item(), sigma=sigma.item(),beta=beta.item(),nugget=nugget.item(),lr=lrr)
            torch._assert(not torch.isnan(nu),"nu is nan")
           
        #
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
        model.eval()
        likelihood.eval()
        means = torch.tensor([0.])
        with torch.no_grad():
            test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)
            for x_batch, y_batch in test_iter:
                preds = model(x_batch)
                means = torch.cat([means, preds.mean.cpu()])
            #
            means = means[1:]
            error = torch.mean(torch.abs(means - test_y.cpu()))

        with torch.no_grad():
            preds = model(test_x)
            l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
            ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
            LB,UB = ci
            LB = torch.as_tensor(LB,dtype=torch.float32, device=torch.device('cpu'))
            UB = torch.as_tensor(UB,dtype=torch.float32, device=torch.device('cpu'))
            a = torch.logical_and(test_y>=LB, test_y<=UB)
            acc = test_y[a].numel() / test_y.numel()
        #
        print(f"Test, MAE: {error.item()} ,  ACC: {acc}, train:{train_y.numel()}, test:{test_y.numel()}")
        if show_ci:
            print(showCI_NSBN(model))
        #
        if best_mae > error.item():
            best_mae = error.item()
            param_fp = os.path.join(folder, f'best_mae.pth')
            torch.save(model.state_dict(), param_fp) 
            print(showCI_NSBN(model))
        #
        ma = {'mae':error.item(), 'acc':acc,}
        records.append(ma)
    return records

# %% ../V00/01_util.ipynb 12
def train_LhtMatern_step(df, varogram_dist, inducing_points,device,prior='gamma',lr=0.01, base_lr=0.0001, lr_step=100,
                         batch_size=200, num_epoch=1, show_ci=True, folder=None,nugget=0.3):
    ''' don't support to use CUDA
    parameters:
    returen
        records: List[dict] training results
    '''
    best_mae = 3
    
    if folder == None:
        torch._assert(False, "need a folder to save the parameters of models")
    points = df.to_numpy()
    voxel_grid = grid_dataset(points,varogram_dist)
    keys = np.array(list(voxel_grid.keys()))
    ur, uc = keys.max(axis=0)
    cv_ds = []
    for k,v in voxel_grid.items():
        ret = {'test':None, 'train':None}
        ret['test'] = v
        r,c = k
        neighbours = findNeighbors(r,c,ur=ur,uc=uc)
        neighbours += [k]
        train = []
        for i,j in voxel_grid.items():
            if i in neighbours:
                continue
            train.append(j.tolist())
        ret['train'] = np.vstack(train)
        cv_ds.append(ret)
    #
    records=[]
    model = lhtcore.GP_AFirst(inducing_points,nu=1, max_tries=200,prior=prior,nugget=nugget)
    likelihood = gpytorch.likelihoods.GaussianLikelihood() 
    model.to(device)
    likelihood.to(device)
    params = list(model.parameters()) + list(likelihood.parameters())
    optimizer = torch.optim.Adam(params, lr = lr, weight_decay=0.01 )
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=lr_step, eta_min=base_lr)
                                                                                   
    if show_ci:
        print(showCI_NSBN(model))
    inds = len(cv_ds)
    a = min(1000,num_epoch*inds)
    cv_iter = tqdm.notebook.tqdm(range(a), desc='cross-validation')

    for fid, i in enumerate(cv_iter):
        test_points = cv_ds[i%inds]['test']
        test_x = test_points[:,0:2]
        test_y = test_points[:,2]
        test_x = torch.from_numpy(test_x).float()
        test_y = torch.from_numpy(test_y).float()
        #
        train_points = cv_ds[i%inds]['train']
        train_x = train_points[:,0:2]
        train_y = train_points[:,2]
        train_x = torch.from_numpy(train_x).float()
        train_y = torch.from_numpy(train_y).float()

        train_dataset = TensorDataset(train_x, train_y)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        model.train()
        likelihood.train()
        mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
        minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
        for x_batch, y_batch in minibatch_iter:
            # Perform Adam step to optimize hyperparameters
            optimizer.zero_grad()
            output = model(x_batch.to(device))
            loss = -mll(output, y_batch.to(device))
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            #
            lrr = optimizer.param_groups[0]['lr']
            nu = model.covar_module.base_kernel.current_nu
            sigma = model.covar_module.base_kernel.current_sigma
            beta = model.covar_module.base_kernel.current_beta
            nugget = model.covar_module.base_kernel.current_nugget
            minibatch_iter.set_postfix(loss=loss.item(), nu=nu.item(), sigma=sigma.item(),beta=beta.item(),nugget=nugget.item(),lr=lrr)
            torch._assert(not torch.isnan(nu),"nu is nan")
           
        #
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
        model.eval()
        likelihood.eval()
        means = torch.tensor([0.])
        with torch.no_grad():
            test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)
            for x_batch, y_batch in test_iter:
                preds = model(x_batch.to(device))
                means = torch.cat([means, preds.mean.cpu()])
            #
            means = means[1:]
            error = torch.mean(torch.abs(means - test_y.cpu()))
        #
        print(f'----Test:{test_points.shape[0]}-----')
        ret_test = lht_test(model,test_points,device,flag=True)
        print(f'----All:{points.shape[0]}-----')
        ret_all = lht_test(model,points,device,flag=True)
        print('-' * 30)
        if show_ci:
            print(showCI_NSBN(model))
        #
        param_fp = os.path.join(folder, f'{fid}.pth')
        torch.save(model.state_dict(), param_fp) 
        ma = {'mae':ret_all['mae'], 'acc':ret_all['acc'], 'param':param_fp,'test_mae':ret_test['mae'],'test_acc':ret_test['acc']}
        records.append(ma)
    return records

# %% ../V00/01_util.ipynb 13
def test_records(df, inducing_points, records, model_type='standard', prior='gamma', cuda=False, mae_threshold=1):
    points = df.to_numpy()
    X = points[:,0:2]
    Y = points[:,2]
    yy = torch.from_numpy(Y).float()
    tx = torch.from_numpy(X).float()
    if cuda:
        yy, tx = yy.cuda(), tx.cuda()
    riter = tqdm.notebook.tqdm(records, desc='verify')
    ret={'mae':[], 'acc':[]}
    for idx,i in enumerate(riter):
        print(f"{idx}: mae: {i['mae']}, acc: {i['acc']}")
        if i['mae'] >=mae_threshold:
            continue
        state_dict_path = i['param']
        state_dict = torch.load(state_dict_path)
        if model_type=='standard':
            model = StandardApproximateGP_mat(inducing_points=inducing_points, nu=2.5)
        else:
            model = GP_AFirst(inducing_points.cpu(), prior=prior)
        model.load_state_dict(state_dict)
        if cuda:
            model = model.cuda()
        else:
            model = model.cpu()
        model.eval()
        
        preds = model(tx)
        means = preds.mean.cpu()
        error = torch.mean(torch.abs(means - yy.cpu()))
        l,v = preds.loc.cpu().detach().numpy(), preds.stddev.cpu().detach().numpy()
        ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
        LB,UB = ci
        a = np.logical_and(Y>=LB, Y<=UB)
        acc = Y[a].shape[0] / len(Y)
        ret['mae'].append(error.item())
        ret['acc'].append(acc)
        print(f"num:{Y[a].shape[0]}, acc:{acc}, mae:{error.item()}")
    return ret

# %% ../V00/01_util.ipynb 14
def lht_test(model,points,device,flag=True):
    model.eval()
    X = points[:,0:2]
    Y = points[:,2]
    yy = torch.from_numpy(Y).float()
    tx = torch.from_numpy(X).float()
    #
    test_dataset = TensorDataset(tx, yy)
    test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)
    test_iter = tqdm.notebook.tqdm(test_loader, desc="test",leave=False)

    means = torch.tensor([0.])
    ls, vs = torch.tensor([0.]), torch.tensor([0.])
    for x_batch, y_batch in test_iter:
        preds = model(x_batch)
        means = torch.cat([means, preds.mean.cpu()])
        ls = torch.cat([ls, preds.loc.cpu()])
        vs = torch.cat([vs, preds.stddev.cpu()])
    #
    ret={'mae':0, 'acc':0}
    means = means[1:]
    error = torch.mean(torch.abs(means - yy.cpu()))
    l,v = ls[1:].detach().numpy(),vs[1:].detach().numpy()
    ci = scipy.stats.norm.interval(0.95, loc=l, scale=v)
    LB,UB = ci
    a = np.logical_and(Y>=LB, Y<=UB)
    acc = Y[a].shape[0] / len(Y)
    ret['mae'] = error.item()
    ret['acc'] = acc
    #
    if flag:
        print(f"num:{Y[a].shape[0]}, acc:{acc}, mae:{error.item()}")
    return ret

# %% ../V00/01_util.ipynb 15
def lht_train_one_epoch_old(model,optimizer,likelihood,points,device,batch_size=200):
    model.train()
    likelihood.train()
    #
    train_x = points[:,0:2]
    train_y = points[:,2]
    train_x = torch.from_numpy(train_x).float()
    train_y = torch.from_numpy(train_y).float()

    train_dataset = TensorDataset(train_x, train_y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
    for x_batch, y_batch in minibatch_iter:
        # Perform Adam step to optimize hyperparameters       
        output = model(x_batch.to(device))
        loss = -mll(output, y_batch)
        #
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        #
        lrr = optimizer.param_groups[0]['lr']
        nu = model.covar_module.base_kernel.current_nu
        sigma = model.covar_module.base_kernel.current_sigma
        beta = model.covar_module.base_kernel.current_beta
        nugget = model.covar_module.base_kernel.current_nugget
        minibatch_iter.set_postfix(loss=loss.item(), nu=nu.item(), sigma=sigma.item(),beta=beta.item(),nugget=nugget.item(),lr=lrr)
        #
        torch._assert(not torch.isnan(nu),"nu is nan")
        torch._assert(not torch.isnan(sigma),"sigma is nan")
        torch._assert(not torch.isnan(beta),"beta is nan")
        torch._assert(not torch.isnan(nugget),"nugget is nan")
        #
    pass

# %% ../V00/01_util.ipynb 16
def lht_train_one_epoch(model,optimizer,likelihood,mll,points,device,lr_scheduler,batch_size=200):
    model.train()
    likelihood.train()
    
    #
    train_x = points[:,0:2]
    train_y = points[:,2]
    train_x = torch.from_numpy(train_x).float()
    train_y = torch.from_numpy(train_y).float()

    train_dataset = TensorDataset(train_x, train_y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    #mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.numel())
    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc="Minibatch", leave=False)
    for x_batch, y_batch in minibatch_iter:
        # Perform Adam step to optimize hyperparameters  
        optimizer.zero_grad()
        output = model(x_batch.to(device))
        loss = -mll(output, y_batch)
        #
        
        loss.backward()
        optimizer.step()
        if lr_scheduler is not None:
            lr_scheduler.step()
        #
        lrr = optimizer.param_groups[0]['lr']
        nu = model.covar_module.base_kernel.current_nu
        sigma = model.covar_module.base_kernel.current_sigma
        beta = model.covar_module.base_kernel.current_beta
        nugget = model.covar_module.base_kernel.current_nugget
        minibatch_iter.set_postfix(loss=loss.item(), nu=nu.item(), sigma=sigma.item(),beta=beta.item(),nugget=nugget.item(),lr=lrr)
        #
        torch._assert(not torch.isnan(nu),"nu is nan")
        torch._assert(not torch.isnan(sigma),"sigma is nan")
        torch._assert(not torch.isnan(beta),"beta is nan")
        torch._assert(not torch.isnan(nugget),"nugget is nan")
        #
    pass
